# Required Skills & Experience

• Strong **AWS** experience: **Redshift/Athena** (databases, tables, partitions, clustering, DML/DDL, UDFs), **S3** (bucket design, lifecycle, security).

• **Apache Airflow**: DAG design, custom operators/hooks, scheduling, backfilling, managing XComs, SLAs, and retries.

• **PySpark**: RDD/DataFrame APIs, performance tuning (partitioning, caching, skew mitigation), reading/writing **Parquet/Avro/JSON/CSV**.

• **Python**: Clean, modular code; packaging; virtual environments; error handling; typing; unit testing (pytest).

• **SQL**: Advanced SQL (window functions, CTEs, optimization via EXPLAIN/QUERY PLAN, data deduplication).

• **Data Modeling**: Dimensional modeling (star/snowflake schemas), normalization/denormalization, fact and dimension tables.

• **ETL/ELT Concepts**: Incremental loads, CDC, SCD types, late-arriving dimensions, schema evolution, idempotency.

• **Testing/Quality**: Data validation frameworks (e.g., Great Expectations or equivalent patterns), synthetic test data, boundary & regression tests.

• **CI/CD for data** (Git, branching, code reviews, deployments to Composer/Airflow, infra-as-code familiarity).
